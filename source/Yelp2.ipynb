{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Yelp2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bIznpCYbF2t6","colab_type":"code","outputId":"82d54bf4-e573-4f10-cfab-6682f7d402c3","executionInfo":{"status":"ok","timestamp":1589297627050,"user_tz":420,"elapsed":29815,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DWaRGFIkHGKF","colab_type":"code","outputId":"893749b6-5e05-4877-c8ee-df9e0c6e4978","executionInfo":{"status":"ok","timestamp":1589297642595,"user_tz":420,"elapsed":20484,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":921}},"source":["!pip install keras-bert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting keras-bert\n","  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.4)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n","Collecting keras-transformer>=0.30.0\n","  Downloading https://files.pythonhosted.org/packages/22/b9/9040ec948ef895e71df6bee505a1f7e1c99ffedb409cb6eb329f04ece6e0/keras-transformer-0.33.0.tar.gz\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n","Collecting keras-pos-embd>=0.10.0\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.22.0\n","  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n","Collecting keras-layer-normalization>=0.12.0\n","  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.7.0\n","  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n","Collecting keras-self-attention==0.41.0\n","  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n","Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=8ee77ff1d3e3731f1f1259b43d179ed0bf05eac1f2ac8d43d082c40ca861b451\n","  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.33.0-cp36-none-any.whl size=13260 sha256=00580cdd9b7917b5269fa2bd3b84607b9910a6ee2dc19a7ed06db4efad4c3a7e\n","  Stored in directory: /root/.cache/pip/wheels/26/98/13/a28402939e1d48edd8704e6b02f223795af4a706815f4bf6d8\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=9da595f81624ab3c52f55067719ed72f06f2d6b6ca1f869c5deb659164a47152\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=261a1425e4bf4be1c8122f1d56fa721c18643dab0a6c127e58871915138abd76\n","  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=7be4d9c5096bffef55efd1e4a5494267567abb67fe1ef24f2df945191207d0fb\n","  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=ed8a3cf4b8a648c801ff5eb8b225ab3a7622f706add0898161485868cbb4caf0\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=b01780f7462889ae4c86b5c29b9e817e032c72aef97e5d8a8fbbadfabdc05190\n","  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=10f466f55e20ec9699afbb430a2c436a5fa0d76f0c50491504e1e2218ccdf84b\n","  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n","Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n","Successfully installed keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.33.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"41OAPWJ1GF0k","colab_type":"code","outputId":"7e5f9f6a-ed1e-4d7c-ace5-2f9604431eb0","executionInfo":{"status":"ok","timestamp":1589297654010,"user_tz":420,"elapsed":7979,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","import sys\n","from collections import Counter\n","import numpy as np\n","import json\n","import os\n","from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential, load_model, Input, Model\n","from keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional, Flatten, Reshape, Concatenate\n","from keras.callbacks import ModelCheckpoint\n","from keras.losses import SparseCategoricalCrossentropy, sparse_categorical_crossentropy, categorical_crossentropy, Loss, MSE\n","from keras.optimizers import Adam\n","from keras_bert import get_model, compile_model\n","from keras_bert.layers import Extract\n","from keras_bert import get_base_dict\n","from keras_bert import Tokenizer as bert_tokenizer\n","from collections import Counter\n","from tabulate import tabulate\n","from multiprocessing import Pool, Queue, Manager\n","from itertools import combinations\n","\n","MY_DRIVE = \"/content/gdrive/My Drive\"\n","root_folder = os.path.join(MY_DRIVE, \"YelpHelp\") # change this depending on the machine (Colab vs IPython)\n","dataset_folder = os.path.join(root_folder, \"datasets\")\n","dataset_name = \"yelp_review_training_dataset.jsonl\"\n","models_dir = os.path.join(root_folder, \"models\")\n","checkpoint_dir = os.path.join(models_dir, \"checkpoints\")\n","tokenizers_dir = os.path.join(models_dir, \"tokenizers\")\n","test_set_dir = os.path.join(root_folder, \"test-sets\")\n","ensemble_dir = os.path.join(root_folder, \"ensembles\")\n","\n","sys.path.append(os.path.join(root_folder, \"source\"))\n","from utils import *\n","from models import *"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"j5fnXh5k3t0d","colab_type":"text"},"source":["Check for GPU"]},{"cell_type":"code","metadata":{"id":"pbPCEO6HOM3R","colab_type":"code","outputId":"97d94228-56d5-411b-f123-010fd3816e7c","executionInfo":{"status":"ok","timestamp":1589297665162,"user_tz":420,"elapsed":8500,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XHlmV1oMHJvM","colab_type":"text"},"source":["Utility functions"]},{"cell_type":"code","metadata":{"id":"I2oXEgAtEAFo","colab_type":"code","colab":{}},"source":["def get_texts_and_labels(dataset):\n","    texts = [d[\"text\"] for d in dataset]\n","    labels = [d[\"stars\"] - 1 for d in dataset]\n","    return texts, labels\n","\n","def load_tokenizer(name):\n","    file_path = os.path.join(tokenizers_dir, name)\n","    with open(file_path) as tkf:\n","        return tokenizer_from_json(tkf.read())\n","\n","def train_model(model, train_seqs, train_labels, num_epochs, save_as, batch_size=64, validation_split=.2, save_weights=False):\n","    save_file = os.path.join(models_dir, save_as)\n","    checkpoint_file = os.path.join(checkpoint_dir, f\"{save_as}.ckpt\")\n","    cp_callback = ModelCheckpoint(filepath=checkpoint_file, verbose=1, save_weights_only=save_weights)\n","    training_result = model.fit(train_seqs, train_labels, epochs=num_epochs, batch_size=batch_size, validation_split=.2, callbacks=[cp_callback])\n","    model.save(save_file)\n","\n","def predict_on_texts(model, texts, preprocessor, actual_stars=None):\n","    inputs = preprocessor.preprocess(texts)\n","    predictions = model.predict(inputs)\n","    for i, p in enumerate(predictions):\n","        print(\"---------------------\")\n","        print(\"TEXT:\\n{}\\nPREDICTED STARS:{}\".format(texts[i], np.argmax(p) + 1))\n","        if actual_stars:\n","            print(\"ACTUAL STARS: {}\".format(actual_stars[i]))\n","\n","def get_balanced_dataset(dataset, size=1000):\n","    class_counter = Counter()\n","    result = []\n","    ration = size // 5\n","    finished = set()\n","    for d in dataset:\n","        star = d[\"stars\"]\n","        if star not in finished:\n","            class_counter[star] += 1\n","            result.append(d)\n","            if class_counter[star] >= ration:\n","                finished.add(star)\n","        if len(finished) == 5:\n","            return result\n","    return result\n","        \n","\n","def predict_from_data(model, dataset, preprocessor):\n","    stars = None\n","    if \"stars\" in dataset[0]:\n","        stars = [d[\"stars\"] for d in dataset]\n","    texts = [d[\"text\"] for d in dataset]\n","    predict_on_texts(model, texts, preprocessor, actual_stars=stars)\n","\n","def batch_predict(batch, model, preprocessor):\n","    texts = [b[\"text\"] for b in batch]\n","    batch_input = preprocessor.preprocess(texts)\n","    predictions = model.predict_ratings(batch_input)\n","    assert len(batch) == len(predictions)\n","    for i, b in enumerate(batch):\n","        b[\"predicted_stars\"] = predictions[i]\n","        \n","def predict_test_set(test_set, model, preprocessor, batch_size=64, show_accuracy=True, print_results=True):\n","    for i in range(0, len(test_set), batch_size):\n","        batch = test_set[i: i + batch_size]\n","        batch_predict(batch, model, preprocessor)\n","    accuracy, avg_star_error = None, None\n","    if show_accuracy:\n","        accuracy = (len([d for d in test_set if d[\"stars\"] == d[\"predicted_stars\"]]) / len(test_set)) * 100\n","        avg_star_error = sum([abs(d[\"predicted_stars\"] - d[\"stars\"]) for d in test_set]) / len(test_set)\n","        if print_results:\n","            print(\"Accuracy: {:.3f}\".format(accuracy))\n","            print(\"Average Star Error: {:.5f}\".format(avg_star_error))\n","    return accuracy, avg_star_error\n","\n","def load_data_set(name, test_set=False):\n","    set_dir = test_set_dir if test_set else dataset_folder\n","    with open(os.path.join(set_dir, name)) as df:\n","        return [json.loads(line) for line in df]\n","\n","def load_keras_model(name, custom_objects={}, compile=True):\n","    return load_model(os.path.join(models_dir, name), custom_objects=custom_objects, compile=compile)\n","\n","def load_custom_model(name, loss_func, custom_objects={}, metrics=[]):\n","    model = load_keras_model(name, custom_objects=custom_objects, compile=False)\n","    model.compile(optimizer=Adam(), loss=loss_func, metrics=metrics)\n","    return model\n","\n","def load_transformer(name):\n","    weights_file = os.path.join(models_dir, name)\n","    model = build_transformer_model()\n","    model.load_weights(weights_file)\n","    return model\n","\n","def compare_class_accuracies(test_set, models_and_preprocs):\n","    results = {}\n","    def class_result(c):\n","        relevant = [d for d in test_set if int(d[\"stars\"]) == c]\n","        acc = len([d for d in relevant if d[\"stars\"] == d[\"predicted_stars\"]]) / len(relevant)\n","        star_err = sum([abs(d[\"stars\"] - d[\"predicted_stars\"]) for d in relevant]) / len(relevant)\n","        return acc, star_err\n","    for name, model_and_preproc in models_and_preprocs.items():\n","        model, preprocessor = model_and_preproc\n","        avg_acc, avg_se = predict_test_set(test_set, model, preprocessor, print_results=False)\n","        result = results[name] = [(avg_acc, avg_se)]\n","        result += [class_result(c) for c in range(1, 6)]\n","    headers = [\"Model\", \"OVERALL\\naccuracy | star error\"]\n","    headers += [\"{}\\naccuracy | star error\".format(star) for star in range(1, 6)]\n","    def format_result(result):\n","        acc, star_error = result\n","        return \"{:.3f}     {:.3f}\".format(acc, star_error)\n","    table = [[name] + [format_result(r) for r in result] for name, result in results.items()]\n","    print(tabulate(table, headers, tablefmt='fancy_grid'))\n","\n","\n","\n","def compare_on_test_sets(test_sets, models_and_preprocs, show_results=True):\n","    results = {name: {} for name in models_and_preprocs}\n","    for model_name, model_and_preproc in models_and_preprocs.items():\n","        result = results[model_name]\n","        overall = result['overall'] = [0, 0]\n","        for test_name, test_set in test_sets.items():\n","            model, preprocessor = model_and_preproc\n","            acc, star_error = predict_test_set(test_set, model, preprocessor, print_results=False)\n","            result[test_name] = [acc, star_error]\n","            overall[0] += acc\n","            overall[1] += star_error\n","        overall[0] /= len(test_sets)\n","        overall[1] /= len(test_sets)\n","    table = []\n","    test_names = list(test_sets.keys())\n","    headers = [\"Model\", \"OVERALL\\nstar error | accuracy\"]\n","    headers += [\"{}\\nstar error | accuracy\".format(name) for name in test_names]\n","    def format_result(result, col_name):\n","        acc, star_error = result\n","        is_best_acc = acc == max([r[col_name][0] for r in results.values()])\n","        is_best_star_error = star_error == min([r[col_name][1] for r in results.values()])\n","        return \"{:.3f}{}     {:.3f}{}\".format(star_error, \" *\" if is_best_acc else \"  \", acc, \" *\" if is_best_star_error else \"  \")\n","    for model_name, result in results.items():\n","        row = [model_name, format_result(result['overall'], 'overall')]\n","        row += [format_result(result[name], name) for name in test_names]\n","        table.append(row)\n","    if show_results:\n","        print(tabulate(table, headers, tablefmt='fancy_grid'))\n","    return results\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUJfGbFuzvIQ","colab_type":"code","colab":{}},"source":["dataset = load_data_set(dataset_name)\n","challenge_3 = load_data_set(\"yelp_challenge_3_with_answers.jsonl\", test_set=True)\n","challenge_5 = load_data_set(\"yelp_challenge_5_with_answers.jsonl\", test_set=True)\n","challenge_6 = load_data_set(\"yelp_challenge_6_with_answers.jsonl\", test_set=True)\n","challenge_8 = load_data_set(\"yelp_challenge_8_with_answers.jsonl\", test_set=True)\n","random_test_set = np.random.choice(dataset, 10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lg31ILI8mESt","colab_type":"code","outputId":"4b6804cc-d6c0-430a-bd64-107b411730c9","executionInfo":{"status":"ok","timestamp":1589254620592,"user_tz":420,"elapsed":704,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["class_counter = Counter()\n","for d in dataset:\n","    class_counter[d[\"stars\"]] += 1\n","weights = []\n","for star, count in sorted(class_counter.items(), key=lambda t: t[0]):\n","    fraction = count / len(dataset)\n","    weight = .1 * (1 / fraction)\n","    weights.append(weight)\n","    print(\"star:\", star, \"fraction:\", fraction, \"weight:\", weight, \"count: \", count)\n","loss_weights = np.array(weights, dtype=\"float32\")\n","print(loss_weights)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["star: 1.0 fraction: 0.2434082173090871 weight: 0.4108324735521028 count:  129878\n","star: 2.0 fraction: 0.06720254281917834 weight: 1.4880389313402869 count:  35858\n","star: 3.0 fraction: 0.0642133059460513 weight: 1.55730963429939 count:  34263\n","star: 4.0 fraction: 0.13572822120727687 weight: 0.7367664521830384 count:  72422\n","star: 5.0 fraction: 0.48944771271840637 weight: 0.204311916066779 count:  261160\n","[0.41083246 1.4880389  1.5573096  0.73676646 0.20431192]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mgrQ4hh9HZV-","colab_type":"text"},"source":["Loss and Model functions"]},{"cell_type":"code","metadata":{"id":"vROMeO4rr3pN","colab_type":"code","colab":{}},"source":["global_indices = tf.constant([0., 1., 2., 3., 4.])\n","def star_squared_error(y_true, y_pred):\n","    indices = tf.reshape(tf.tile(global_indices, [tf.shape(y_pred)[0]]), tf.shape(y_pred))\n","    true_indices = tf.squeeze(y_true, axis=1)\n","    weighted = y_pred * indices\n","    weighted_avgs = tf.reduce_sum(weighted, axis=1)\n","    return (weighted_avgs - true_indices) ** 2\n","\n","def weighted_loss(loss_func, weights=[2., 5., 5., 3., 1.]):\n","    loss_weights = tf.constant(weights)\n","    def weighted_loss_func(y_true, y_pred):\n","        true_indices = tf.cast(tf.squeeze(y_true, axis=1), tf.int32)\n","        one_hots = tf.one_hot(true_indices, depth=5, dtype=tf.float32)\n","        weight_vec = tf.linalg.matvec(one_hots, loss_weights)\n","        return weight_vec * loss_func(y_true, y_pred)\n","    return weighted_loss_func\n","\n","\n","def hybrid_loss(weighting=[.5, .5]):\n","    entropy_weighting, error_weighting = weighting\n","    def loss_func(y_true, y_pred):\n","        entropy_loss = sparse_categorical_crossentropy(y_true, y_pred)\n","        star_loss = star_squared_error(y_true, y_pred)\n","        return entropy_weighting * entropy_loss + error_weighting * star_loss\n","    return loss_func\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrPHD2MEr_-_","colab_type":"code","colab":{}},"source":["labels = tf.constant([[2.], [4.]], dtype=tf.float32)\n","preds = tf.constant([\n","                     [1., 0., 0., 0., 0.],\n","                     [0, 0, 1, 0, 0]\n","], dtype=tf.float32)\n","print(star_squared_error(labels, preds).numpy())\n","# print(weighted_star_loss(labels, preds).numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdguDMPk6R1c","colab_type":"code","colab":{}},"source":["def build_model(input_length=150, rnn_size=256, loss='scc', use_glove=False, vocab_size=50000, \n","                learning_rate=1e-3, dropout_rate=.2, use_gru=True, use_bidirectional=True, \n","                use_c2v=False, show_accuracy=True, hybrid_weighting=[.5, .5]):\n","    model = Sequential()\n","    if not use_c2v:\n","        if use_glove:\n","            embed = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)\n","        else:\n","            embed = Embedding(vocab_size, rnn_size, input_length=input_length)\n","        model.add(embed)\n","    if use_gru:\n","        rnn_cell = GRU(rnn_size, dropout=dropout_rate)\n","    else:\n","        rnn_cell = LSTM(rnn_size, dropout=dropout_rate)\n","    if use_bidirectional:\n","        model.add(Bidirectional(rnn_cell))\n","    else:\n","        model.add(rnn_cell)\n","    model.add(Dense(5, activation='softmax'))\n","    if loss == 'scc':\n","        loss_func = sparse_categorical_crossentropy\n","    elif loss == 'star':\n","        loss_func = star_squared_error\n","    elif loss == 'hybrid':\n","        loss_func = hybrid_loss(hybrid_weighting)\n","    elif loss == 'wsl':\n","        loss_func = weighted_star_loss\n","    optimizer = Adam(learning_rate=learning_rate)\n","    metrics = ['sparse_categorical_accuracy'] if show_accuracy else []\n","    model.compile(optimizer=optimizer, loss=loss_func, metrics=metrics)\n","    return model\n","\n","def build_char_model(input_length=150, word_length=5, word_embedding_dim=100, \n","                     char_embedding_dim=10, use_glove=False, vocab_size=50000, \n","                     char_vocab_size=72, learning_rate=1e-3, dropout_rate=.2, \n","                     use_gru=True, use_bidirectional=True, use_c2v=False, loss='scc',\n","                     show_accuracy=True, weight_loss=False, loss_weights=[2., 5., 5., 3., 1.]):\n","    word_inputs = Input(shape=(input_length,))\n","    char_inputs = Input(shape=(input_length, word_length))\n","    flattened_chars = Flatten()(char_inputs)\n","    if not use_c2v:\n","        if use_glove:\n","            embed = Embedding(vocab_size, word_embedding_dim, weights=[embedding_matrix], trainable=False)\n","        else:\n","            embed = Embedding(vocab_size, word_embedding_dim, input_length=input_length)\n","        word_embeddings = embed(word_inputs)\n","        flattened_character_embeddings = Embedding(char_vocab_size, char_embedding_dim, input_length=word_length * input_length)(flattened_chars)\n","        character_embeddings = Reshape((input_length, word_length * char_embedding_dim))(flattened_character_embeddings)\n","    embeddings = Concatenate()([word_embeddings, character_embeddings])\n","    rnn_size = word_embedding_dim + word_length * char_embedding_dim\n","    if use_gru:\n","        rnn_cell = GRU(rnn_size, dropout=dropout_rate)\n","    else:\n","        rnn_cell = LSTM(rnn_size, dropout=dropout_rate)\n","    if use_bidirectional:\n","        rnn_out = Bidirectional(rnn_cell)(embeddings)\n","    else:\n","        rnn_out = rnn_cell(embeddings)\n","    logits = Dense(5, activation='softmax')(rnn_out)\n","    model = Model([word_inputs, char_inputs], logits)\n","    if loss == 'scc':\n","        loss_func = sparse_categorical_crossentropy\n","    elif loss == 'star':\n","        loss_func = star_squared_error\n","    elif loss == 'hybrid':\n","        loss_func = hybrid_loss(hybrid_weighting)\n","    if weight_loss:\n","        loss_func = weighted_loss(loss_func, loss_weights)\n","    optimizer = Adam(learning_rate=learning_rate)\n","    metrics = ['sparse_categorical_accuracy'] if show_accuracy else []\n","    model.compile(optimizer=optimizer, loss=loss_func, metrics=metrics)\n","    return model\n","\n","def build_transformer_model(num_transformers=6, learning_rate=1e-3):\n","    def weighted_loss(loss_func, weights=[2., 5., 5., 3., 1.]):\n","        loss_weights = tf.constant(weights)\n","        def weighted_loss_func(y_true, y_pred):\n","            weight_vec = tf.linalg.matvec(y_true, loss_weights)\n","            return weight_vec * loss_func(y_true, y_pred)\n","        return weighted_loss_func\n","    inputs, output_layer = get_model(\n","        token_num=50000,\n","        head_num=5,\n","        transformer_num=num_transformers,\n","        embed_dim=100,\n","        feed_forward_dim=100,\n","        seq_len=150,\n","        pos_num=150,\n","        dropout_rate=0.05,\n","        training=False,\n","        trainable=True,\n","        output_layer_num=1\n","    )\n","\n","    extract_layer = Extract(index=0, name='Extract')(output_layer)\n","    feed_forward_1 = Dense(units=100, name=\"feed_forward_1\")(extract_layer)\n","    output_logits = Dense(\n","        units=5,\n","        activation='softmax',\n","        name='NSP',\n","    )(feed_forward_1)\n","\n","    model = Model(inputs, [output_logits])\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss=weighted_loss(categorical_crossentropy), metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITnmBsia-A52","colab_type":"code","colab":{}},"source":["class YelpModel:\n","    def __init__(self, keras_model):\n","        self.keras_model = keras_model\n","\n","    def predict_ratings(self, preprocessed_inputs):\n","        return [np.argmax(p) + 1 for p in self.keras_model.predict(preprocessed_inputs)]\n","\n","    def predict(self, preprocessed_inputs):\n","        return self.keras_model.predict(preprocessed_inputs)\n","\n","class EnsembleModel(YelpModel):\n","    def __init__(self, config):\n","        self.num_models = len(config)\n","        self.models = [model for model, _ in config]\n","        self.weights = [weight for _, weight in config]\n","            \n","    # averages the softmax probabilites\n","    def predict_ratings(self, preprocessed_inputs):\n","        assert len(preprocessed_inputs) == self.num_models\n","        num_samples = len(preprocessed_inputs[0])\n","        if num_samples == 2:   # dumbass hard code to fix char inputs - np.ma.size(..., axis=-2) didn't work\n","            num_samples = len(preprocessed_inputs[0][0])\n","        predictions = np.zeros((num_samples, 5))\n","        for i, inputs in enumerate(preprocessed_inputs):\n","            predictions += self.weights[i] * self.models[i].predict(inputs)\n","        return [np.argmax(p) + 1 for p in predictions]\n","\n","    def all_probs(self, preprocessed_inputs):\n","        return np.array([self.models[i].predict(pi) for i, pi in enumerate(preprocessed_inputs)])\n","\n","    def copy(self):\n","        clone = EnsembleModel([])\n","        clone.models = self.models\n","        clone.weights = self.weights.copy()\n","        clone.num_models = self.num_models\n","        return clone\n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YD_qnj0twgss","colab_type":"code","colab":{}},"source":["class YelpPreprocessor:\n","    def preprocess(self, texts):\n","        raise NotImplementedError # abstract class\n","\n","class SimpleTokenizerPadder(YelpPreprocessor):\n","    def __init__(self, tokenizer, input_length=150):\n","        self.tokenizer = tokenizer\n","        self.input_length = input_length\n","    def preprocess(self, texts):\n","        return pad_sequences(self.tokenizer.texts_to_sequences(texts), maxlen=self.input_length)\n","\n","class CharacterModelPreprocessor(YelpPreprocessor):\n","    def __init__(self, word_tokenizer, char_tokenizer, input_length=150, word_length=5):\n","        self.word_tokenizer = word_tokenizer\n","        self.input_length = input_length\n","        self.char_tokenizer = char_tokenizer\n","        self.word_length = word_length\n","    def character_preprocess(self, texts):\n","        char_sequences = self.char_tokenizer.texts_to_sequences(texts)\n","        out = np.zeros((len(char_sequences), self.input_length, self.word_length))\n","        space_character = self.char_tokenizer.word_index[' ']\n","        for i, seq in enumerate(char_sequences):\n","            word_index = 0\n","            char_index = 0\n","            for c in char_sequences[i]:\n","                if c == space_character:\n","                    if char_index != 0:\n","                        word_index += 1\n","                    char_index = 0\n","                else:\n","                    if char_index < self.word_length:\n","                        out[i, word_index, char_index] = c\n","                    char_index += 1\n","                if word_index >= self.input_length:\n","                    break\n","            if word_index < self.input_length:\n","                adj = 1 if char_index != 0 else 0 # if char_index is 0, we added one at the end, and word_index = num_words, else we are at example index 1 in a len 5, and we want to roll 3, not 4\n","                out[i] = np.roll(out[i], self.input_length - word_index - adj, axis=0)\n","        return out\n","    \n","    def word_preprocess(self, texts):\n","        return pad_sequences(self.word_tokenizer.texts_to_sequences(texts), maxlen=self.input_length)\n","\n","    def preprocess(self, texts):\n","        return [self.word_preprocess(texts), self.character_preprocess(texts)]\n","\n","class BertTokenizer(YelpPreprocessor):\n","    def __init__(self, tokenizer, input_length=150):\n","        self.tokenizer = tokenizer\n","        self.input_length = input_length\n","    def preprocess(self, texts):\n","        sequences = np.zeros((len(texts), self.input_length))\n","        segments = np.zeros((len(texts), self.input_length))\n","        for i, text in enumerate(texts):\n","            sequences[i], segments[i] = self.tokenizer.encode(text, max_len=self.input_length)\n","            sequences[sequences > 100000] = 1 \n","        return [sequences, segments]    \n","\n","class EnsemblePreprocessor(YelpPreprocessor):\n","    def __init__(self, preprocessors):\n","        self.preprocessors = preprocessors\n","\n","    def preprocess(self, texts):\n","        return [p.preprocess(texts) for p in self.preprocessors]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5krvH7ZbAEq","colab_type":"text"},"source":["Load in all the tokenizers and models"]},{"cell_type":"code","metadata":{"id":"OsrkW9xLscZM","colab_type":"code","outputId":"7e30e479-0777-40c8-b424-29eb0507af74","executionInfo":{"status":"ok","timestamp":1589297803065,"user_tz":420,"elapsed":64520,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["texts, labels = get_texts_and_labels(dataset)\n","tokenizer_50000 = load_tokenizer(\"test_tokenizer_50000\")\n","tokenizer_50000_with_unks = load_tokenizer(\"test_tokenizer_50000_with_unks\")\n","tokenizer_100000 = load_tokenizer(\"test_tokenizer_100000\")\n","tokenizer_100000_with_unks = load_tokenizer(\"test_tokenizer_100000_with_unks\")\n","char_tk = load_tokenizer(\"test_char_tokenizer\")\n","new_token_dict = get_base_dict()\n","for word, i in tokenizer_100000_with_unks.word_index.items():\n","    if word != 'UNK':\n","        if i + 3 < 50000:\n","            new_token_dict[word] = i + 3\n","transformer_tokenizer = bert_tokenizer(new_token_dict)\n","\n","\n","preprocessor = SimpleTokenizerPadder(tokenizer_50000)\n","glove_preprocessor = SimpleTokenizerPadder(tokenizer_100000, input_length=300)\n","glove_char_preprocessor = CharacterModelPreprocessor(tokenizer_100000_with_unks, char_tk, input_length=300)\n","char_preprocessor = CharacterModelPreprocessor(tokenizer_50000_with_unks, char_tk)\n","bert_preprocessor = BertTokenizer(transformer_tokenizer)\n","\n","weighted_star_loss = weighted_loss(star_squared_error)\n","\n","glove_gru_bi = YelpModel(load_keras_model(\"glove_gru_bi\"))\n","glove_gru_bi_char = YelpModel(load_keras_model(\"glove_gru_bi_char\"))\n","gru_bi_50000 = YelpModel(load_keras_model(\"gru_bi_50000\"))\n","gru_bi_50000_star_loss = YelpModel(load_custom_model(\"gru_bi_50000_star_loss\", star_squared_error, metrics=['sparse_categorical_accuracy']))\n","gru_bi_50000_wsl = YelpModel(load_custom_model(\"gru_bi_50000_wsl\", weighted_star_loss, metrics=['sparse_categorical_accuracy']))\n","gru_bi_50000_hl = YelpModel(load_custom_model(\"gru_bi_50000_hybrid_loss\", hybrid_loss()))\n","gru_bi_char = YelpModel(load_keras_model(\"gru_bi_char\"))\n","gru_bi_char_wscc = YelpModel(load_custom_model(\"gru_bi_char_wscc\", weighted_loss(sparse_categorical_crossentropy)))\n","bert_model = YelpModel(load_transformer(\"checkpoints/bert_model_6_wscc_epoch_11.h5\"))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"2whi8r8ZjLMR","colab_type":"text"},"source":["Set up ensemble"]},{"cell_type":"code","metadata":{"id":"f2On-RlxTWZO","colab_type":"code","colab":{}},"source":["def generate_weights(num_models, depth=.05):\n","    divisor = int(1 / depth)\n","    def generate_helper(current_weights, left):\n","        if sum(current_weights) > divisor:\n","            return\n","        elif left <= 1:\n","            for w in range(21):\n","                if w + sum(current_weights) == divisor:\n","                    yield [weight / divisor for weight in current_weights + [w]]\n","        else:\n","            for w in range(21):\n","                yield from generate_helper(current_weights + [w], left - 1)\n","    yield from generate_helper([], num_models)\n","\n","def best_weights_given_probs(probs, labels):\n","    bests = {'acc': [0, []], 'err': [5, []], 'score': [-100, []]}\n","    num_samples = len(probs[0])\n","    for weights in generate_weights(len(probs)):\n","        average_probs = np.average(probs, axis=0, weights=weights)\n","        predictions = np.argmax(average_probs, axis=1)\n","        acc = np.sum(predictions == labels) / num_samples\n","        star_err = np.sum(np.abs(predictions - labels)) / num_samples\n","        score = acc - star_err\n","        if acc > bests['acc'][0]:\n","            bests['acc'] = [acc, weights]\n","        if star_err < bests['err'][0]:\n","            bests['err'] = [star_err, weights]\n","        if score > bests['score'][0]:\n","            bests['score'] = [score, weights]\n","    return bests\n","\n","def best_weights(ensemble_model, ensemble_preproc, test_set):\n","    texts, labels = get_texts_and_labels(test_set)\n","    inputs = ensemble_preproc.preprocess(texts)\n","    probs = ensemble_model.all_probs(inputs)\n","    return best_weights_given_probs(probs, labels)\n","\n","def save_bests(bests, save_name):\n","    with open(os.path.join(ensemble_dir, save_name), \"w+\") as snf:\n","        print(json.dumps(bests), file=snf)\n","\n","def load_bests(save_name):\n","    with open(os.path.join(ensemble_dir, save_name)) as snf:\n","        return json.load(snf)\n","\n","def get_mps_for_bests(ensemble, preproc, names):\n","    models_and_preprocs = {}\n","    bests = [load_bests(name) for name in names]\n","    for name, best in zip(names, bests):\n","        for met in ('ACC', 'ERR', 'SCORE'):\n","            cur_ens = ensemble.copy()\n","            cur_ens.weights = best[met.lower()][1]\n","            models_and_preprocs[f\"{name}_{met}\"] = (cur_ens, preproc)\n","    return models_and_preprocs\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkIZj2sbpn-p","colab_type":"text"},"source":["HERE IS THE BIG ENSEMBLE, UNWEIGHTED"]},{"cell_type":"code","metadata":{"id":"JygQHdMbpnSM","colab_type":"code","colab":{}},"source":["all_models =          [\n","                          (glove_gru_bi, 0.), (glove_gru_bi_char, 0.),\n","                          (gru_bi_50000, 0.), (gru_bi_50000_star_loss, 0.), (gru_bi_50000_wsl, 0.),\n","                          (gru_bi_char, 0.), (gru_bi_char_wscc, 0.),\n","                          (bert_model, 0.)\n","                      ]\n","ensemble = EnsembleModel(all_models)\n","#bigger_ensemble = EnsembleModel(all_models)\n","\n","\n","ensemble_preproc = EnsemblePreprocessor([glove_preprocessor, glove_char_preprocessor,\n","                                         preprocessor, preprocessor, preprocessor,\n","                                         char_preprocessor, char_preprocessor,\n","                                         bert_preprocessor])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1V9YNeDapCRz","colab_type":"text"},"source":["CHANGE THE TEST SETS AND RUN THIS BLOCK TO GET THE BEST WEIGHTS"]},{"cell_type":"code","metadata":{"id":"z1fvkx4fF-Fk","colab_type":"code","outputId":"c71f3417-fd33-4e2d-e2fb-0cdddf19e2d9","executionInfo":{"status":"ok","timestamp":1589300206314,"user_tz":420,"elapsed":2364474,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["if True:\n","    bests = best_weights(ensemble, ensemble_preproc, random_test_set)\n","    name = \"big_ensemble_random_test_set_results\"\n","    # save it \n","    save_bests(bests, name)\n","    print(bests)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'acc': [0.8982, [0.0, 0.0, 0.35, 0.1, 0.0, 0.05, 0.0, 0.5]], 'err': [0.1192, [0.0, 0.0, 0.4, 0.1, 0.0, 0.0, 0.0, 0.5]], 'score': [0.779, [0.0, 0.0, 0.4, 0.1, 0.0, 0.0, 0.0, 0.5]]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qckNb_59ewPm","colab_type":"code","outputId":"f4719173-7758-49dd-bc8a-3dffdba74472","executionInfo":{"status":"ok","timestamp":1589300348051,"user_tz":420,"elapsed":2499219,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["names = [\"big_ensemble_random_test_set_results\"]\n","mps = get_mps_for_bests(ensemble, ensemble_preproc, names)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, mps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["╒════════════════════════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╕\n","│ Model                                      │ OVERALL                 │ CHALLENGE 3             │ CHALLENGE 5             │ CHALLENGE 6             │ CHALLENGE 8             │\n","│                                            │ star error | accuracy   │ star error | accuracy   │ star error | accuracy   │ star error | accuracy   │ star error | accuracy   │\n","╞════════════════════════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╡\n","│ big_ensemble_random_test_set_results_ACC   │ 0.958       49.233      │ 0.551 *     53.933 *    │ 0.682 *     38.800 *    │ 2.114       40.000      │ 0.486       64.200 *    │\n","├────────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_random_test_set_results_ERR   │ 0.952 *     49.583 *    │ 0.552 *     53.933      │ 0.686 *     38.800      │ 2.080 *     41.200 *    │ 0.488 *     64.400      │\n","├────────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_random_test_set_results_SCORE │ 0.952 *     49.583 *    │ 0.552 *     53.933      │ 0.686 *     38.800      │ 2.080 *     41.200 *    │ 0.488 *     64.400      │\n","╘════════════════════════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╛\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_B64fpHHpfGg","colab_type":"text"},"source":["CHANGE THE NAMES AND RUN TO TEST IT"]},{"cell_type":"code","metadata":{"id":"DbK3oeJ0IETs","colab_type":"code","outputId":"4ef836ec-c7a2-4580-b383-ad551aa72a3b","executionInfo":{"status":"ok","timestamp":1589242342444,"user_tz":420,"elapsed":761187,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["names = [\"big_ensemble_results\", \"big_ensemble_no_challenge_5_results\", \"big_ensemble_challenges_3_8_results\"]\n","mps = get_mps_for_bests(ensemble, ensemble_preproc, names)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, mps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'acc': [0.5575221238938053, [0.05, 0.1, 0.05, 0.05, 0.4, 0.1, 0.25, 0.0]], 'err': [0.8254670599803343, [0.0, 0.05, 0.2, 0.15, 0.55, 0.0, 0.0, 0.05]], 'score': [-0.272369714847591, [0.0, 0.0, 0.0, 0.15, 0.15, 0.15, 0.4, 0.15]]}\n","{'acc': [0.5932203389830508, [0.05, 0.15, 0.2, 0.0, 0.25, 0.2, 0.15, 0.0]], 'err': [0.9015645371577575, [0.0, 0.0, 0.3, 0.15, 0.35, 0.2, 0.0, 0.0]], 'score': [-0.31551499348109513, [0.05, 0.1, 0.2, 0.05, 0.25, 0.2, 0.15, 0.0]]}\n","{'acc': [0.6595744680851063, [0.0, 0.45, 0.1, 0.1, 0.15, 0.0, 0.2, 0.0]], 'err': [0.3916827852998066, [0.2, 0.05, 0.0, 0.1, 0.05, 0.05, 0.5, 0.05]], 'score': [0.2659574468085106, [0.1, 0.3, 0.05, 0.05, 0.0, 0.15, 0.3, 0.05]]}\n","╒═══════════════════════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╕\n","│ Model                                     │ OVERALL                 │ CHALLENGE 3             │ CHALLENGE 5             │ CHALLENGE 6             │ CHALLENGE 8             │\n","│                                           │ star error | accuracy   │ star error | accuracy   │ star error | accuracy   │ star error | accuracy   │ star error | accuracy   │\n","╞═══════════════════════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╡\n","│ big_ensemble_results_ACC                  │ 0.838 *     55.605      │ 0.380       64.419 *    │ 0.532 *     51.800 *    │ 2.002       41.000      │ 0.436       65.200      │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_results_ERR                  │ 0.833       55.158 *    │ 0.390       64.232      │ 0.552       49.200      │ 1.958       41.800      │ 0.432       65.400      │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_results_SCORE                │ 0.835       55.446      │ 0.408       61.985      │ 0.556       50.200      │ 1.950       42.800      │ 0.426       66.800      │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_no_challenge_5_results_ACC   │ 0.847       55.245      │ 0.384 *     64.981      │ 0.620       43.400      │ 1.974       45.600      │ 0.412       67.000      │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_no_challenge_5_results_ERR   │ 0.846       54.390      │ 0.421       62.360      │ 0.646       42.600      │ 1.908 *     46.000 *    │ 0.408       66.600      │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_no_challenge_5_results_SCORE │ 0.843       55.111      │ 0.393       64.045      │ 0.618       43.600      │ 1.958       45.600      │ 0.402       67.200 *    │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_challenges_3_8_results_ACC   │ 0.894       53.402      │ 0.390       64.607      │ 0.684       37.600      │ 2.092       44.000      │ 0.410 *     67.400      │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_challenges_3_8_results_ERR   │ 0.859       54.668      │ 0.382       63.670      │ 0.588       46.400      │ 2.062       41.200      │ 0.402 *     67.400 *    │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_challenges_3_8_results_SCORE │ 0.883       54.049      │ 0.380       64.794 *    │ 0.650       40.800      │ 2.094       43.400      │ 0.408       67.200      │\n","╘═══════════════════════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╛\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hAkBSL1HCSQZ","colab_type":"code","outputId":"d3e2b449-a3cb-4dd1-b6e3-c4ce59744368","executionInfo":{"status":"ok","timestamp":1589257981642,"user_tz":420,"elapsed":205043,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":309}},"source":["names = [\"big_ensemble_results\", \"big_ensemble_no_challenge_5_results\"]\n","mps = get_mps_for_bests(ensemble, ensemble_preproc, names)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_class_accuracies(challenge_3 + challenge_5 + challenge_6 + challenge_8, mps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["╒═══════════════════════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╕\n","│ Model                                     │ OVERALL                 │ 1                       │ 2                       │ 3                       │ 4                       │ 5                       │\n","│                                           │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │\n","╞═══════════════════════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╡\n","│ big_ensemble_results_ACC                  │ 55.752     0.830        │ 0.624     1.003         │ 0.534     0.514         │ 0.545     0.500         │ 0.604     0.416         │ 0.522     1.301         │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_results_ERR                  │ 55.310     0.825        │ 0.596     1.053         │ 0.512     0.531         │ 0.562     0.489         │ 0.560     0.460         │ 0.560     1.221         │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_results_SCORE                │ 55.556     0.828        │ 0.624     1.020         │ 0.517     0.542         │ 0.494     0.534         │ 0.660     0.356         │ 0.524     1.269         │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_no_challenge_5_results_ACC   │ 55.408     0.840        │ 0.652     1.033         │ 0.453     0.603         │ 0.551     0.534         │ 0.492     0.524         │ 0.618     1.170         │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_no_challenge_5_results_ERR   │ 54.523     0.839        │ 0.629     1.078         │ 0.436     0.635         │ 0.528     0.573         │ 0.456     0.568         │ 0.641     1.076         │\n","├───────────────────────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ big_ensemble_no_challenge_5_results_SCORE │ 55.261     0.835        │ 0.644     1.051         │ 0.451     0.605         │ 0.539     0.539         │ 0.496     0.520         │ 0.621     1.142         │\n","╘═══════════════════════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╛\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dMe9D5u2L1EV","colab_type":"code","colab":{}},"source":["texts, labels = get_texts_and_labels(challenge_3 + challenge_5 + challenge_6 + challenge_8)\n","ensemble.weights = [0.05, 0.15, 0.2, 0.0, 0.25, 0.2, 0.15, 0.0]\n","inputs = ensemble_preproc.preprocess(texts)\n","predictions = np.asarray(ensemble.predict_ratings(inputs)) - 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"okNuCM5hNm0I","colab_type":"code","outputId":"61ef8196-7b95-450d-c569-5ea6e983a803","executionInfo":{"status":"ok","timestamp":1589259089890,"user_tz":420,"elapsed":292,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(predictions)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[3 2 0 ... 4 4 4]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gu28YfgINKTs","colab_type":"code","outputId":"a4a2ca92-f403-4598-befe-8d1abb6b8f8b","executionInfo":{"status":"ok","timestamp":1589259092602,"user_tz":420,"elapsed":240,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["import sklearn\n","confusion_matrix = sklearn.metrics.confusion_matrix(labels, predictions)\n","print(confusion_matrix)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[258  36  12  11  79]\n"," [224 274  82  16   9]\n"," [  7  18  98  47   8]\n"," [  1   2  24 123 100]\n"," [151   6  12  62 374]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5KvUkM9MkTHg","colab_type":"text"},"source":["                                        0.395\t       0.633\t     0.540\t      0.518\t    1.994     \t0.418\t      0.458\t       0.634\t\t"]},{"cell_type":"markdown","metadata":{"id":"gX3fyIEBAwNb","colab_type":"text"},"source":["OLD STUFF:\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O8_7MRuN90S5","colab_type":"text"},"source":["Store a mapping of names to ensembles for easy loading"]},{"cell_type":"code","metadata":{"id":"Q11-8Yj14A1I","colab_type":"code","colab":{}},"source":["mapping = {\n","    \"GRU_BI\": (gru_bi_50000, preprocessor),\n","    \"GRU_BI_WSL\": (gru_bi_50000_wsl, preprocessor),\n","    \"GRU_BI_CHAR\": (gru_bi_char, char_preprocessor),\n","    \"GRU_BI_CHAR_WSCC\": (gru_bi_char_wscc, char_preprocessor),\n","    \"BERT_MODEL\": (bert_model, bert_preprocessor)\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WXxbsHVzv90","colab_type":"code","colab":{}},"source":["models_and_preprocs = get_mps_for_all_bests(mapping)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, models_and_preprocs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64SzuIQe9TxV","colab_type":"text"},"source":["Find the best ensemble over all challenge sets"]},{"cell_type":"code","metadata":{"id":"JBR2gPzT6R10","colab_type":"code","colab":{}},"source":["with open(os.path.join(ensemble_dir, \"all_results\"), \"w+\") as ar:\n","    print(json.dumps(results), file=ar)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uskzVvyCRate","colab_type":"code","colab":{}},"source":["mpns = [\n","          (gru_bi_50000, preprocessor, \"GRU_BI\"),\n","          (gru_bi_50000_wsl, preprocessor, \"GRU_BI_WSL\"),\n","          (gru_bi_char, char_preprocessor, \"GRU_BI_CHAR\"),\n","          (gru_bi_char_wscc, char_preprocessor, \"GRU_BI_CHAR_WSCC\"),\n","          (bert_model, bert_preprocessor, \"BERT_MODEL\")\n","]\n","if True: # I already ran this, so I'll just load it from file\n","    bests = best_ensemble(mpns, challenge_3 + challenge_6 + challenge_8, avg_predictions=True)\n","    save_bests(bests, \"challenge_3_6_8_avg_preds_emsemble_bests\")\n","else:\n","    bests = load_bests(\"all_tests_ensemble_bests\", ensemble_mapping)\n","print(bests)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Val3gZPj9abb","colab_type":"text"},"source":["Save the results to a json file, and test the ensembles (one for each metric - err is star error and score is accuracy / 100 - star error)"]},{"cell_type":"code","metadata":{"id":"If2sR7s1XnF2","colab_type":"code","outputId":"ce9c0ef3-577f-474c-f0b6-1fae819fff6f","executionInfo":{"status":"ok","timestamp":1589222274439,"user_tz":420,"elapsed":25587,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["models_and_preprocs = models_and_preprocs_from_bests(bests, avg_predictions=True)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, models_and_preprocs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["╒═════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╕\n","│ Model               │ OVERALL                 │ CHALLENGE 3             │ CHALLENGE 5             │ CHALLENGE 6             │ CHALLENGE 8             │\n","│                     │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │\n","╞═════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╡\n","│ BEST_ACC_ENSEMBLE   │ 48.919       0.950      │ 60.674       0.464      │ 27.000       0.812      │ 44.000       2.030      │ 64.000 *     0.494      │\n","├─────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ BEST_ERR_ENSEMBLE   │ 50.277       0.873 *    │ 63.109 *     0.404 *    │ 45.000 *     0.608 *    │ 31.400       2.010      │ 61.600       0.468 *    │\n","├─────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ BEST_SCORE_ENSEMBLE │ 52.528 *     0.891      │ 60.112       0.434      │ 42.400       0.660      │ 44.200 *     1.994 *    │ 63.400       0.474      │\n","╘═════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╛\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CTER-h9it41l","colab_type":"code","colab":{}},"source":["models_and_preprocs = get_mps_for_all_bests(ensemble_mapping)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, models_and_preprocs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDsdBnBH97LO","colab_type":"text"},"source":["Find the best ensemble over every challenge except 5 (bc why not)"]},{"cell_type":"code","metadata":{"id":"OVLhcZrA5Nbe","colab_type":"code","outputId":"cf2b79b8-92fc-4d95-fcf1-397bdf6e90a7","executionInfo":{"status":"ok","timestamp":1589213707818,"user_tz":420,"elapsed":267,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["new_bests = load_bests(\"no_challenge_5_bests\", ensemble_mapping)\n","print(new_bests)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'acc': [0.5775749674054759, <__main__.EnsembleModel object at 0x7f1734b8c0b8>, <__main__.EnsemblePreprocessor object at 0x7f1734b8ccc0>, [0.2, 0.1, 0.5, 0.2], 'GRU_BI-GRU_BI_WSL-GRU_BI_CHAR-GRU_BI_CHAR_WSCC'], 'err': [0.9250325945241199, <__main__.EnsembleModel object at 0x7f1734b8cd68>, <__main__.EnsemblePreprocessor object at 0x7f1734b8cc88>, [0.25, 0.4, 0.35], 'GRU_BI-GRU_BI_WSL-GRU_BI_CHAR'], 'score': [-0.3487614080834419, <__main__.EnsembleModel object at 0x7f1734b8cd68>, <__main__.EnsemblePreprocessor object at 0x7f1734b8cc88>, [0.25, 0.4, 0.35], 'GRU_BI-GRU_BI_WSL-GRU_BI_CHAR']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZeK7OFyp7qVF","colab_type":"code","outputId":"773d1546-cbf6-42f3-af30-dca432723ff0","executionInfo":{"status":"ok","timestamp":1589213782948,"user_tz":420,"elapsed":54380,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["models_and_preprocs = models_and_preprocs_from_bests(new_bests)\n","test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, models_and_preprocs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["╒═════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╕\n","│ Model               │ OVERALL                 │ CHALLENGE 3             │ CHALLENGE 5             │ CHALLENGE 6             │ CHALLENGE 8             │\n","│                     │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │\n","╞═════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╡\n","│ BEST_ACC_ENSEMBLE   │ 54.121     0.871        │ 63.483     0.404        │ 43.600     0.640        │ 43.800     2.020        │ 65.600     0.420        │\n","├─────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ BEST_ERR_ENSEMBLE   │ 54.287     0.856        │ 62.547     0.410        │ 44.600     0.612        │ 44.400     1.974        │ 65.600     0.426        │\n","├─────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ BEST_SCORE_ENSEMBLE │ 54.287     0.856        │ 62.547     0.410        │ 44.600     0.612        │ 44.400     1.974        │ 65.600     0.426        │\n","╘═════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╛\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VRZ9utKd8n7a","colab_type":"code","outputId":"39adf329-69d2-4a5e-817d-5fb3059fe258","executionInfo":{"status":"ok","timestamp":1589170716157,"user_tz":420,"elapsed":313,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(load_bests(\"all_tests_ensemble_bests\", ensemble_mapping))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'acc': [0.5521140609636185, <__main__.EnsembleModel object at 0x7fdde7f30160>, <__main__.EnsemblePreprocessor object at 0x7fdde7f30358>, [0.1, 0.4, 0.3, 0.2], 'GRU_BI-GRU_BI_WSL-GRU_BI_CHAR-GRU_BI_CHAR_WSCC'], 'err': [0.8352999016715831, <__main__.EnsembleModel object at 0x7fdde7f30160>, <__main__.EnsemblePreprocessor object at 0x7fdde7f30358>, [0.25, 0.55, 0.15, 0.05], 'GRU_BI-GRU_BI_WSL-GRU_BI_CHAR-GRU_BI_CHAR_WSCC'], 'score': [-0.2846607669616519, <__main__.EnsembleModel object at 0x7fdde7f30160>, <__main__.EnsemblePreprocessor object at 0x7fdde7f30358>, [0.05, 0.4, 0.4, 0.15], 'GRU_BI-GRU_BI_WSL-GRU_BI_CHAR-GRU_BI_CHAR_WSCC']}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KlrNozja75ml","colab_type":"text"},"source":["Here are all of the best ensembles hardcoded, for posterity"]},{"cell_type":"code","metadata":{"id":"9C8cLQIJdgp1","colab_type":"code","colab":{}},"source":["best_acc_ensemble_config = [\n","    (gru_bi_50000, .1),\n","    (gru_bi_50000_wsl, .4),\n","    (gru_bi_char, .3),\n","    (gru_bi_char_wscc, .2)\n","]\n","\n","best_err_ensemble_config = [\n","    (gru_bi_50000, .25),\n","    (gru_bi_50000_wsl, .55),\n","    (gru_bi_char, .15),\n","    (gru_bi_char_wscc, .05)\n","]\n","\n","best_score_ensemble_config = [\n","    (gru_bi_50000, .05),\n","    (gru_bi_50000_wsl, .4),\n","    (gru_bi_char, .4),\n","    (gru_bi_char_wscc, .15)\n","]\n","\n","best_acc_ensemble = EnsembleModel(best_acc_ensemble_config)\n","best_err_ensemble = EnsembleModel(best_err_ensemble_config)\n","best_score_ensemble = EnsembleModel(best_score_ensemble_config)\n","best_ensemble_preprocessor = EnsemblePreprocessor([preprocessor, preprocessor, char_preprocessor, char_preprocessor])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-3j9dkJV1KE","colab_type":"code","colab":{}},"source":["models_and_preprocs = {\n","                        \"BEST_ACC_ENSEMBLE\": (best_acc_ensemble, best_ensemble_preprocessor),\n","                        \"BEST_ERR_ENSEMBLE\": (best_err_ensemble, best_ensemble_preprocessor),\n","                        \"BEST_SCORE_ENSEMBLE\": (best_score_ensemble, best_ensemble_preprocessor)\n","                      }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yq0mRK7zt78i","colab_type":"code","outputId":"d033ca08-8f47-4a8b-e7d3-6d90e8159185","executionInfo":{"status":"ok","timestamp":1589167431872,"user_tz":420,"elapsed":93875,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["test_sets = {\"CHALLENGE 3\": challenge_3, \"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6, \"CHALLENGE 8\": challenge_8}\n","results = compare_on_test_sets(test_sets, models_and_preprocs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["╒═════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╤═════════════════════════╕\n","│ Model               │ OVERALL                 │ CHALLENGE 3             │ CHALLENGE 5             │ CHALLENGE 6             │ CHALLENGE 8             │\n","│                     │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │ accuracy | star error   │\n","╞═════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╪═════════════════════════╡\n","│ BEST_ACC_ENSEMBLE   │ 55.074     0.847        │ 63.296     0.395        │ 51.800     0.540        │ 41.800     1.994        │ 63.400     0.458        │\n","├─────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ BEST_ERR_ENSEMBLE   │ 54.724     0.843        │ 63.296     0.395        │ 50.200     0.546        │ 42.000     1.964        │ 63.400     0.466        │\n","├─────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┼─────────────────────────┤\n","│ BEST_SCORE_ENSEMBLE │ 54.980     0.843        │ 62.921     0.397        │ 51.000     0.552        │ 42.200     1.976        │ 63.800     0.448        │\n","╘═════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╧═════════════════════════╛\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-eTIzcIVVVBP","colab_type":"code","colab":{}},"source":["train_data, testing_data = train_test_split(dataset, train_size=.8)\n","balanced_test_set = get_balanced_dataset(testing_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIrIMiSKVytn","colab_type":"code","colab":{}},"source":["compare_class_accuracies(balanced_test_set, models_and_preprocs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBDML4wHLMzS","colab_type":"code","colab":{}},"source":["from keras.utils import to_categorical"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j27YxM3pvCCF","colab_type":"code","colab":{}},"source":["train_seqs, test_seqs, train_labels, test_labels = train_test_split(preprocessor.preprocess(texts), labels, train_size=.8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzXlYqhmyD6a","colab_type":"code","colab":{}},"source":["processed_words, processed_chars = char_preprocessor.preprocess(texts)\n","train_seq_words, test_seq_words, train_seq_chars, test_seq_chars, train_labels, test_labels = train_test_split(processed_words, \n","                                                                                                               processed_chars, labels, train_size=.8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhrvUddnKZi6","colab_type":"code","colab":{}},"source":["start = np.random.randint(0, 400000)\n","fake_test_set = [{\"text\": text, \"stars\": stars + 1} for text, stars in zip(texts[start:start + 10000], labels[start:start + 10000])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHtWY5UFvW-8","colab_type":"code","outputId":"101f8507-66fc-4d1e-9dc5-afddb17eb54e","executionInfo":{"status":"ok","timestamp":1588990560248,"user_tz":420,"elapsed":8738680,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["train_model(gru_bi_char_wscc, [train_seq_words, train_seq_chars], train_labels, 2, \"gru_bi_char_wscc\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 341491 samples, validate on 85373 samples\n","Epoch 1/2\n","341491/341491 [==============================] - 4366s 13ms/step - loss: 1.8082 - sparse_categorical_accuracy: 0.7083 - val_loss: 1.6806 - val_sparse_categorical_accuracy: 0.7064\n","\n","Epoch 00001: saving model to /content/gdrive/My Drive/YelpHelp/models/checkpoints/gru_bi_50000_wscc.ckpt\n","Epoch 2/2\n","341491/341491 [==============================] - 4368s 13ms/step - loss: 1.5277 - sparse_categorical_accuracy: 0.7482 - val_loss: 1.6237 - val_sparse_categorical_accuracy: 0.7386\n","\n","Epoch 00002: saving model to /content/gdrive/My Drive/YelpHelp/models/checkpoints/gru_bi_50000_wscc.ckpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NrWsIvxtwF-2","colab_type":"code","outputId":"1424d6e3-8f5d-4991-b37e-705fa8ec2aa3","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["train_model(gru_bi_50000_wsl, train_seqs, train_labels, 2, \"gru_bi_wsl\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 341491 samples, validate on 85373 samples\n","Epoch 1/2\n","301248/341491 [=========================>....] - ETA: 8:27 - loss: 0.6980 - sparse_categorical_accuracy: 0.7512"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yPiVdso-Kyse","colab_type":"code","outputId":"91a1d000-ed22-4865-f59d-8c56ab09f4f1","executionInfo":{"status":"ok","timestamp":1589073882327,"user_tz":420,"elapsed":22119,"user":{"displayName":"Ajay Raj","photoUrl":"","userId":"17684825857785761020"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["loss, accuracy = bert_model.evaluate(bert_preprocessor.preprocess(fake_test_set), to_categorical(np.asarray(fake_test_set_labels)), batch_size=64)\n","print(f\"Loss: {loss}\\tAccuracy: {accuracy}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 3s 295us/step\n","Loss: 0.4607791851043701\tAccuracy: 0.8310999870300293\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gFazArribsp6","colab_type":"code","outputId":"ef45ef83-4ea9-431f-f833-22ecbb40e33c","executionInfo":{"status":"ok","timestamp":1588977404622,"user_tz":420,"elapsed":4525,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["gru_bi_char = load_keras_model(\"gru_bi_char\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NKJbQSllZIS8","colab_type":"code","outputId":"331cca64-7b08-46c5-9869-c0236188cc4d","executionInfo":{"status":"ok","timestamp":1588978143661,"user_tz":420,"elapsed":6436,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["test_sets = {\"CHALLENGE 5\": challenge_5, \"CHALLENGE 6\": challenge_6}\n","models_and_preprocs = {\"GRU_BI\": (gru_bi_50000_wsl, preprocessor), \"GRU_BI_CHAR\": (gru_bi_char, char_preprocessor)}\n","compare_on_test_sets(test_sets, models_and_preprocs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CHALLENGE 5:\n","-------------------\n","GRU_BI:\n","======\n","Accuracy: 62.200\n","Average Star Error: 0.42200\n","======\n","\n","GRU_BI_CHAR:\n","======\n","Accuracy: 42.400\n","Average Star Error: 0.66000\n","======\n","\n","CHALLENGE 6:\n","-------------------\n","GRU_BI:\n","======\n","Accuracy: 30.400\n","Average Star Error: 2.03800\n","======\n","\n","GRU_BI_CHAR:\n","======\n","Accuracy: 44.200\n","Average Star Error: 1.99400\n","======\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zDvW8NRD6dmc","colab_type":"text"},"source":["Try some models"]},{"cell_type":"markdown","metadata":{"id":"lEv8MEtLk_RV","colab_type":"text"},"source":["Some examples"]},{"cell_type":"code","metadata":{"id":"EIs-a0vok9fb","colab_type":"code","colab":{}},"source":["ex1 = \"The place was pretty decent. We got seated quickly and the service was pretty good. It was a bit too dark and loud for me, but that's just my preference. I would definitely come back here again\"\n","ex2 = \"Chuck-e-cheeze makes its competitors look like trash. Dave and busters: shit. Nobody is as good as chuck-e-cheeze. I will not be going anywhere else for a good time.\"\n","ex3 = \"I lobe this plase. Evertime I com here its greate. Will for sure come agan@\"\n","text_examples = [ex1, ex2, ex3]\n","\n","# random examples\n","random_samples = np.random.choice(dataset, 5)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jc2BWty0LsNx","colab_type":"code","outputId":"00d0e051-2936-473f-eeda-1359fee0b03e","executionInfo":{"status":"ok","timestamp":1588976352670,"user_tz":420,"elapsed":25822,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["mini_test_set = dataset[:10000]\n","predict_test_set(mini_test_set, gru_bi_50000_wsl, preprocessor)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy: 70.120\n","Average Star Error: 0.33660\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WlsHN9dbNeu4","colab_type":"code","colab":{}},"source":["predict_from_data(gru_bi_50000_hybrid_loss, mini_test_set, tokenizer_50000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBeKlCcXJgFj","colab_type":"code","outputId":"94e531ae-c7c1-4920-9ce3-2a6ce2fa4760","executionInfo":{"status":"ok","timestamp":1588976566427,"user_tz":420,"elapsed":882,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["predict_on_texts(gru_bi_50000_wsl, text_examples, preprocessor)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["---------------------\n","TEXT:\n","The place was pretty decent. We got seated quickly and the service was pretty good. It was a bit too dark and loud for me, but that's just my preference. I would definitely come back here again\n","PREDICTED STARS:4\n","---------------------\n","TEXT:\n","Chuck-e-cheeze makes its competitors look like trash. Dave and busters: shit. Nobody is as good as chuck-e-cheeze. I will not be going anywhere else for a good time.\n","PREDICTED STARS:3\n","---------------------\n","TEXT:\n","I lobe this plase. Evertime I com here its greate. Will for sure come agan@\n","PREDICTED STARS:4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vFb2v-JPJEvQ","colab_type":"text"},"source":["Play around with chars2vec"]},{"cell_type":"code","metadata":{"id":"dg97Si9h0zXJ","colab_type":"code","outputId":"6fb3fe58-075e-4c17-d628-c67573dfa23b","executionInfo":{"status":"ok","timestamp":1588725503708,"user_tz":420,"elapsed":7779,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!pip install chars2vec"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting chars2vec\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/0a/8c327aae23e0532d239ec7b30446aca765eb5d9547b4c4b09cdd82e49797/chars2vec-0.1.7.tar.gz (8.1MB)\n","\u001b[K     |████████████████████████████████| 8.1MB 4.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: chars2vec\n","  Building wheel for chars2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chars2vec: filename=chars2vec-0.1.7-cp36-none-any.whl size=8111096 sha256=5eb8672ddc46237932fced4d74bfbd9db1027a6401fc078fa917babcc4c8e73a\n","  Stored in directory: /root/.cache/pip/wheels/97/b6/65/d7e778ef1213ec77d315aea0f536068b96e36cc94c02abbfde\n","Successfully built chars2vec\n","Installing collected packages: chars2vec\n","Successfully installed chars2vec-0.1.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ENMS6p33JLYa","colab_type":"code","colab":{}},"source":["import chars2vec as c2v\n","c2v_100 = c2v.load_model('eng_100')\n","input_length = 300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lX_gkXf9lF--","colab_type":"code","outputId":"2e40bc85-055f-446c-afa9-e1ed9e33fcf9","executionInfo":{"status":"ok","timestamp":1588725617962,"user_tz":420,"elapsed":1107,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sample = dataset[0][\"text\"].split()\n","vectors = c2v_100.vectorize_words(sample, maxlen_padseq=input_length)\n","vectors.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(39, 100)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"xHE4NNMzt_ug","colab_type":"code","outputId":"6e492f04-858f-4c34-f67b-10d7056da6c3","executionInfo":{"status":"ok","timestamp":1588364553695,"user_tz":420,"elapsed":12299,"user":{"displayName":"Connor Lafferty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBskLWGuIJTtP2Kf_VuY9aBjfJkB9lMrAXoEb4=s64","userId":"10265635664311828086"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["glove_dir = os.path.join(root_folder, \"source\", \"glove\")\n","glove_file, embedding_dim = os.path.join(glove_dir, \"glove.6B.100d.txt\"), 100 # embedding dim should match file name\n","glove_mappings = {}\n","with open(glove_file) as gf:\n","    for line in gf:\n","        parts = line.split()\n","        word = parts[0]\n","        vec = np.asarray(parts[1:], dtype='float32')\n","        glove_mappings[word] = vec\n","embedding_matrix = np.zeros((max_vocab_len, embedding_dim))\n","found, not_found = 0, 0\n","for word, i in tokenizer.word_index.items():\n","    if i >= max_vocab_len:\n","        continue\n","    vec = glove_mappings.get(word)\n","    if vec is not None:\n","        embedding_matrix[i] = vec\n","        found += 1\n","    else:\n","        not_found += 1\n","print(f\"Found {found} words\")\n","print(f\"Couldn't find {not_found} words\")\n","print(len(glove_mappings))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 67209 words\n","Couldn't find 32790 words\n","400000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FEp9rTKNwkrC","colab_type":"code","colab":{}},"source":["# old dumb stuff\n","def test_weight(mps, weights_chunk, q, test_sets):\n","    print(\"helloo\")\n","    for ws in weights_chunk:\n","        ensemble = EnsembleModel([(mps[i][0], ws[i]) for i in range(len(mps))])\n","        temp = {\"temp\": (ensemble, preproc)}\n","        results = compare_on_test_sets(test_sets, temp, show_results=False)\n","        results['weights'] = ws\n","        q.put(results)\n","        acc, star_err = results[\"temp\"][\"overall\"]\n","        print(\"weights:\", ws, \"accuracy:\", acc, \"star-error:\", star_err)\n","\n","def find_best_weighting_synch(mps, test_sets):\n","  # WIP\n","    bests = {'acc': [0, []], 'err': [5, []], 'score': [-1000, []]}\n","    m = Manager()\n","    q = m.Queue()\n","    preproc = EnsemblePreprocessor([mp[1] for mp in mps])\n","    weights = list(generate_weights(len(mps)))\n","    chunk_size = 20\n","    weight_chunks = [weights[i: i + chunk_size] for i in range(0, len(weights), chunk_size)]\n","    chunks = [(mps, wc, q, test_sets) for wc in weight_chunks]\n","    with Pool(processes=os.cpu_count()) as pool:\n","        pool.starmap(test_weight, chunks)\n","    while not q.empty():\n","        results = q.get()\n","        acc, star_err = results[\"temp\"][\"overall\"]\n","        weights = results[\"weights\"]\n","        score = acc / 100 - star_err\n","        if acc > bests['acc'][0]:\n","            bests['acc'] = [acc, weights]\n","        if star_err < bests['err'][0]:\n","            bests['err'] = [star_err, weights]\n","        if score > bests['score'][0]:\n","            bests['score'] = [score, weights]\n","    return bests\n","\n","def find_best_weighting(models_and_preprocs, test_sets, save_as, start_from_file=False):\n","    save_file = os.path.join(test_set_dir, save_as)\n","    bests = {'acc': [0, []], 'err': [5, []], 'score': [-1000, []], 'cur': 0}\n","    preproc = EnsemblePreprocessor([mp[1] for mp in models_and_preprocs])\n","    all_weights = list(generate_weights(num_models=len(models_and_preprocs)))\n","    if start_from_file:\n","        with open(save_file) as sf:\n","            bests = json.load(sf)\n","    weights = all_weights[bests['cur']:]\n","    for ws in weights:\n","        ensemble = EnsembleModel([(models_and_preprocs[i][0], ws[i]) for i in range(len(ws))])\n","        temp = {\"temp\": (ensemble, preproc)}\n","        results = compare_on_test_sets(test_sets, temp, show_results=False)\n","        acc, star_err = results[\"temp\"][\"overall\"]\n","        score = acc / 100 - star_err\n","        if acc > bests['acc'][0]:\n","            bests['acc'] = [acc, ws]\n","        if star_err < bests['err'][0]:\n","            bests['err'] = [star_err, ws]\n","        if score > bests['score'][0]:\n","            bests['score'] = [score, ws]\n","        bests['cur'] += 1\n","        with open(save_file, 'w+') as sf:\n","            print(json.dumps(bests), file=sf)\n","    return bests"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXTSlMxy3TgW","colab_type":"code","colab":{}},"source":["bert_ensemble_config = [ # best weights\n","    (gru_bi_50000_wsl, .4),\n","    (gru_bi_char_wscc, .45),\n","    (bert_model, .15)\n","]\n","\n","char_ensemble_config = [  \n","    (gru_bi_50000_wsl, .45),\n","    (gru_bi_char_wscc, .3),\n","    (gru_bi_char, .25)\n","]\n","\n","cec2 = [  # best weights\n","    (gru_bi_50000_wsl, .3),\n","    (gru_bi_char_wscc, .4),\n","    (gru_bi_char, .3)\n","]\n","\n","full_ensemble_config = [\n","    (gru_bi_50000_wsl, .25),\n","    (gru_bi_char_wscc, .25),\n","    (gru_bi_char, .25),\n","    (bert_model, .25)\n","]\n","fec2 = [\n","    (gru_bi_50000_wsl, .2),\n","    (gru_bi_char_wscc, .2),\n","    (gru_bi_char, .4),\n","    (bert_model, .2)\n","]\n","\n","bert_ensemble = EnsembleModel(bert_ensemble_config)\n","char_ensemble = EnsembleModel(char_ensemble_config)\n","char_ensemble2 = EnsembleModel(cec2)\n","full_ensemble = EnsembleModel(full_ensemble_config)\n","full_ensemble2 = EnsembleModel(fec2)\n","bert_ensemble_preprocessor = EnsemblePreprocessor([preprocessor, char_preprocessor, bert_preprocessor])\n","char_ensemble_preprocessor = EnsemblePreprocessor([preprocessor, char_preprocessor, char_preprocessor])\n","full_ensemble_preprocessor = EnsemblePreprocessor([preprocessor, char_preprocessor, char_preprocessor, bert_preprocessor])\n","\n","\n","\n","\n","\n","\n","\n","def get_best_weights(self, preprocessed_inputs, labels, weights_generator):\n","        if self.avg_predictions:\n","            return self.get_best_weights1(preprocessed_inputs, labels, weights_generator)\n","        else:\n","            return self.get_best_weights2(preprocessed_inputs, labels, weights_generator)\n","\n","    def get_best_weights1(self, preprocessed_inputs, labels, weights_generator):\n","        bests = {'acc': [0, []], 'err': [5, []], 'score': [-1000, []]}\n","        num_samples = len(preprocessed_inputs[0])\n","        if num_samples == 2:   # dumbass hard code to fix char inputs - np.ma.size(..., axis=-2) didn't work\n","            num_samples = len(preprocessed_inputs[0][0])\n","        probs = np.zeros((self.num_models, num_samples, 5))\n","        for i, inputs in enumerate(preprocessed_inputs):\n","            probs[i] = self.models[i].predict(inputs)\n","        all_predictions = np.argmax(probs, axis=2)\n","        for weights in weights_generator:\n","            average_preds = np.average(all_predictions, axis=0, weights=weights)\n","            predictions = np.around(average_preds)\n","            acc = np.sum(predictions == labels) / num_samples\n","            star_err = np.sum(np.abs(predictions - labels)) / num_samples\n","            score = acc - star_err\n","            if acc > bests['acc'][0]:\n","                bests['acc'] = [acc, weights]\n","            if star_err < bests['err'][0]:\n","                bests['err'] = [star_err, weights]\n","            if score > bests['score'][0]:\n","                bests['score'] = [score, weights]\n","        return bests\n","\n","    def get_best_weights2(self, preprocessed_inputs, labels, weights_generator):\n","        assert len(preprocessed_inputs) == self.num_models\n","        bests = {'acc': [0, []], 'err': [5, []], 'score': [-1000, []]}\n","        num_samples = len(preprocessed_inputs[0])\n","        if num_samples == 2:   # dumbass hard code to fix char inputs - np.ma.size(..., axis=-2) didn't work\n","            num_samples = len(preprocessed_inputs[0][0])\n","        probs = np.zeros((self.num_models, num_samples, 5))\n","        for i, inputs in enumerate(preprocessed_inputs):\n","            probs[i] = self.models[i].predict(inputs)\n","        for weights in weights_generator:\n","            average_probs = np.average(probs, axis=0, weights=weights)\n","            predictions = np.argmax(average_probs, axis=1)\n","            acc = np.sum(predictions == labels) / num_samples\n","            star_err = np.sum(np.abs(predictions - labels)) / num_samples\n","            score = acc - star_err\n","            if acc > bests['acc'][0]:\n","                bests['acc'] = [acc, weights]\n","            if star_err < bests['err'][0]:\n","                bests['err'] = [star_err, weights]\n","            if score > bests['score'][0]:\n","                bests['score'] = [score, weights]\n","        return bests\n","\n","\n","        # averages the predictions\n","    def predict_ratings1(self, preprocessed_inputs):\n","        assert len(preprocessed_inputs) == self.num_models\n","        num_samples = len(preprocessed_inputs[0])\n","        if num_samples == 2:   # dumbass hard code to fix char inputs - np.ma.size(..., axis=-2) didn't work\n","            num_samples = len(preprocessed_inputs[0][0])\n","        predictions = np.zeros((self.num_models, num_samples, 5))\n","        for i, inputs in enumerate(preprocessed_inputs):\n","            predictions[i] = self.models[i].predict(inputs)\n","        stars = np.argmax(predictions, axis=2) + 1\n","        assert stars.shape == (self.num_models, num_samples)\n","        average_per_model = np.average(stars, axis=0, weights=self.weights)\n","        assert average_per_model.shape == (num_samples,)\n","        return np.around(average_per_model).astype(int)\n","\n","\n","\n","name = \"big_ensemble_no_challenge_5_results\"\n","with open(os.path.join(ensemble_dir, name)) as ber:\n","    bests = json.load(ber)\n","ensemble.weights = bests['acc'][1]\n","err_ensemble = ensemble.copy()\n","err_ensemble.weights = bests['err'][1]\n","score_ensemble = ensemble.copy()\n","score_ensemble.weights = bests['score'][1]\n","mps1 = {\"BEST_ACC_ENSEMBLE\": (ensemble, ensemble_preproc), \"BEST_ERR_ENSEMBLE\": (err_ensemble, ensemble_preproc), \"BEST_SCORE_ENSEMBLE\": (score_ensemble, ensemble_preproc)}\n","\n","\n","\n","\n","def best_ensemble(mpns, test_set, show_results=True, avg_predictions=False):\n","    bests = {'acc': [0, None, None, [], \"\"], 'err': [5, None, None, [], \"\"], 'score': [-100, None, None, [], \"\"]}\n","    mets = ('acc', 'err', 'score')\n","    for n in range(1, len(mpns) + 1):\n","        for group in combinations(mpns, n):\n","            ensemble = EnsembleModel([(mpn[0], 0.) for mpn in group], avg_predictions=avg_predictions)\n","            preproc = EnsemblePreprocessor([mpn[1] for mpn in group])\n","            name = \"-\".join([mpn[2] for mpn in group])\n","            result = best_weights(ensemble, preproc, test_set) \n","            acc, err, score = [result[met][0] for met in mets]\n","            if show_results:\n","                print(\"==============\\n{}:\\n\".format(name))\n","                print(\"accuracy: {:.3f}, star-error: {:.3f}, score: {:.3f}\\n\".format(acc, err, score))\n","            acc_weights, err_weights, score_weights = [result[met][1] for met in mets]\n","            if acc > bests['acc'][0]:\n","                bests['acc'] = [acc, ensemble, preproc, acc_weights, name]\n","            if err < bests['err'][0]:\n","                bests['err'] = [err, ensemble, preproc, err_weights, name]\n","            if score > bests['score'][0]:\n","                bests['score'] = [score, ensemble, preproc, score_weights, name]\n","    return bests\n","\n","def models_and_preprocs_from_bests(bests, avg_predictions=False):\n","    models_and_preprocs = {}\n","    for met in ('ACC', 'ERR', 'SCORE'):\n","        _, model, preproc, weights, name = bests[met.lower()]\n","        model_clone = model.copy()\n","        model_clone.weights = weights.copy()\n","        model_clone.num_models = model.num_models\n","        if avg_predictions:\n","            model_clone.predict_ratings = model_clone.predict_ratings1\n","        models_and_preprocs[f\"BEST_{met}_ENSEMBLE\"] = (model_clone, preproc)\n","    return models_and_preprocs\n","\n","\n","ensemble_preproc = EnsemblePreprocessor([glove_preprocessor, glove_char_preprocessor,\n","                                         preprocessor, preprocessor, preprocessor,\n","                                         char_preprocessor, char_preprocessor,\n","                                         bert_preprocessor])\n","\n"],"execution_count":0,"outputs":[]}]}